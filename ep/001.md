# 第 1 回：論文紹介 Don't Stop Pretraining: Adapt Language Models to Domains and Tasks

収録日：20200913

発表者：[@yohei_kikuta](https://twitter.com/yohei_kikuta)  
聞き手：[@smochi_pub](https://twitter.com/smochi_pub)

内容：
- ACL2020 の Honorable Mention Papers に選ばれた論文
- この論文を読もうと思ったのは pretrained model からタスク特化の model を作成する時に現時点で何が分かってて何が分かってないのかを知りたかったため
- 論文のイントロ
  - BERT 以降 NLP でも pre-trained model がめちゃ使われるようになったけど、ドメインに特化してさらに pre-training するのが有効かは十分には研究されてない
  - BERT の後継の RoBERTa を用いて、特定ドメインでさらに pre-training する場合と更に狭い特定タスクで pre-training する場合をいくつかのデータセットで検証
  - ここで言う pre-tranining は unsupervised に MLM を使って学習する、という意味
- 使用データ
  - BIOMED(論文), CS(論文), News, Amazon reviews
  - scispaCy や spaCy を用いて文分割して同じような学習量になるようにパラメタ調整して学習に使用
  - pre-training の効果を見るために classification 用のデータもある
- DAPT と TAPT
  - domain-adaptive pretraining (DAPT) はそのドメインの大量のデータを使って pre-train する
  - task-adaptive pre-training (TAPT) はもっと特化して classification タスクに使うデータ（学習データ）を pre-training に使う
  - 前者はスクレイピングなど頑張ればデータを集められるが学習は大変、後者はより特化してるので少数データでも使えるがデータを集めるのは本質的に難しい
  - 併用もできて併用するのが性能は良い（それはそう）
  - 主たる結果は論文の Table 5 になる。5 回異なる seed で実験して標準偏差も記載するというのは結構よく目にするようになった
  - 違うドメインのデータで pre-training をすると harmful になり得る
- TAPT がデータ量の割に性能が高いのでこれをいい感じに使いたい
  - Human Curated-TAPT は元々ラベル付きのデータを作る際に、母集団からサンプルして作ってる場合、サンプルされなかったデータはタスクとしては同じ用途で集められてるのでこれを使おうという話
  - このような状況は限定的だと思うが、Table 7 にあるように単なる DAPT+TAPT と同じくらいの性能を出せる
  - うまいことこういうデータがない場合にも使いたいので bag-of-words based の embedding を作る VAMPIRE を用いて task の sentence に近いものを k-nearest neighbors で取ってくる
  - 効果は random よりは明確にあるけど、データを絞ってる分 DAPT には及ばず。著者としてはこれで DAPT に勝てるというのを期待してとは思うが、そこまではいかないので、計算量とのトレードオフで選択肢を選べるようになってるぞという感じかな
- What is a domain? というセクションがあって期待させられるが、様々な粒度でスペクトルを成してるんじゃないですかねぇというお気持ち表明
- まとめ

参考情報：
- 原論文：https://arxiv.org/abs/2004.10964
- @yohei_kikuta のこの論文を読んだときのメモ：https://github.com/yoheikikuta/paper-reading/issues/53
- @yohei_kikuta の RoBERTa を読んだときのメモ：https://github.com/yoheikikuta/paper-reading/issues/31
- VAMPIRE：https://github.com/allenai/vampire
- Allen Institute：https://alleninstitute.org
