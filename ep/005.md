# 第 5 回： Text-to-Text Transfer Transformer (T5) 関連の論文を読んだ

収録日：20201109

発表者：[@yohei_kikuta](https://twitter.com/yohei_kikuta)  
聞き手：[@yag_ays](https://twitter.com/yag_ays)  

内容：
- multilingual T5 の論文が出たのを見て、そもそも T5 の論文をちゃんと読んでいなかったことを思い出したので、合わせて読んでみることにした
  - T5 は Text-to-Text Transfer Tranformer の略で、あらゆるタスクの出力を text で扱うことで同じモデルであらゆるタスクに対応しようというモデル
  - T5 は 2019/11 の ACL網羅的サーベイ報告会 で [@kyoun](https://twitter.com/kyoun) さんの講演で聞いてなるほどな〜と思ったけど、論文はちゃんと読めていなかったのでこの機会に読んでみた
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer の紹介
  - これが T5 の原論文で、主題は T5 モデルの提案ではなくて主たるアイデアの実験的比較と考察によりこの分野の現状を理解して今後の展望を提示することにある、とのこと
  - 実験的比較の基礎となるモデルとなる T5 の構造
    - 基本は Encoder-Decoder Transformer で、layer normalization を入れる位置や relative position embedding を使っているところが original とは異なっている
    - 出力をテキストにすることで、様々なタスクを同じモデルで解くことを可能にしている。分類問題なら分類カテゴリのテキストを出力し、回帰問題でさえ離散化した上で数値のテキストを出力する
    - どの問題を解いているかをモデルに教えるために、入力文章には `translate English to German:` などの prefix text をつけることで、学習や予測を可能にする
  - データセット
    - pretraining には Colossal Clean Crawled Corpus (C4) というウェブ上のテキストデータを収集・前処理したものを作成。なんと 750[GB]
    - downstream タスクとしては GLUE, SuperGLUE, SQuAD, WMT などを使用。モデルの性能はこの downstream タスクの結果で測る
  - 実験結果
    - 実験は architecture, unsuprevised objectives, mask の仕方, unlabeled dataset, fine-tuning で学習する層, multi-task learining, scaling, の観点を個別に比較して何が優れているかを大規模に比較
    - 全部は紹介しきれないので、architecture と unsupervised objectives を紹介。前者は Encoder-Decoder, Language Model, Prefix Language Model を比較して Encoder-Decoder が優れていて、後者は BERT のように mask した token を当てるのがいいけど効率性のために全文出力でなくて mask した token の復元のみを出力するのが優れていると判明した
    - 時間があればデータセットの話と multi-task learning の話もする
    - 詳しくは @yohei_kikuta の論文読みを参照
    - 最後に知見を全部詰め込んだモデルで性能を検証。その当時の SoTA を数多く塗り替えた。一方で翻訳タスクでは奮わず、これは英語中心のデータでのみ pretrainig したからだと記載されている
- mT5: A massively multilingual pre-trained text-to-text transformer の紹介
  - T5 はほぼ英語（ドイツ語とかフランス語とかもある程度は対応可能）だけを対象にしていたモデルだが、データを増やすだけで多言語対応ができるモデルなので多言語版を作ったという論文
  - モデル差分は Dropout を使わないとか GeGLU を使う、くらいで大きな変更はない
  - C4 データの多言語バージョン mC4 を構築して、学習時にはデータが少ない言語からのサンプリング割合を増やすように工夫
    - 同じ Common Crawl の多言語データなら Facebook の CC-100 の方が言語別にダウンロードしやすくて便利かも
  - 多言語版の XQuAD や NER タスクなどで SoTA を達成
    - WMT のタスクやってないのなんでだろう？
- T5 と mT5 のソースコードをちょっと読んでみた
  - モデル構造は Mesh TensorFlow というモデル並列の分散学習などがしやすいフレームワークで実装されている
  - Colaboratory の notebook なども準備されているが、11B パラメタのモデルはデカすぎて fine-tuining ができない :innocent:
  - C4 や mC4 は TensorFlow Datasets に取り込まれている 


参考情報：

- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer：[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)
- mT5: A massively multilingual pre-trained text-to-text transformer：[https://arxiv.org/abs/2010.11934](https://arxiv.org/abs/2010.11934)
- @yohei_kikuta が論文を読んだ時のメモ：[https://github.com/yoheikikuta/paper-reading/issues/55](https://github.com/yoheikikuta/paper-reading/issues/55)
-  2019/11 の ACL網羅的サーベイ報告会：[https://nlpaper-challenge.connpass.com/event/152676](https://nlpaper-challenge.connpass.com/event/152676)
- T5 のソースコード：[https://github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)
- mT5 のソースコード：[https://github.com/google-research/multilingual-t5](https://github.com/google-research/multilingual-t5)
- TensorFlow C4 dataset：[https://www.tensorflow.org/datasets/catalog/c4#c4multilingual_nights_stay](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual_nights_stay)
- CC-100：[http://data.statmt.org/cc-100/](http://data.statmt.org/cc-100/)